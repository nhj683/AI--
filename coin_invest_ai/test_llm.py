"""
ë¡œì»¬ LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
try:
    import torch
    import transformers
    from transformers import AutoModelForCausalLM, AutoTokenizer
    PACKAGES_AVAILABLE = True
except ImportError as e:
    PACKAGES_AVAILABLE = False
    MISSING_PACKAGE = str(e)

from config import QWEN_MODEL_PATH


def test_llm_connection():
    """ë¡œì»¬ LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ë¡œì»¬ LLM ì—°ê²° í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 0. í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    if not PACKAGES_AVAILABLE:
        print(f"\nâŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print(f"   ì˜¤ë¥˜: {MISSING_PACKAGE}")
        print(f"\nğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print(f"   pip install -r requirements.txt")
        print(f"\n   ë˜ëŠ” ê°œë³„ ì„¤ì¹˜:")
        print(f"   pip install torch transformers accelerate sentencepiece")
        return False
    
    # 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    print(f"\n1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸...")
    try:
        import torch
        import transformers
        print(f"   âœ… PyTorch ë²„ì „: {torch.__version__}")
        print(f"   âœ… Transformers ë²„ì „: {transformers.__version__}")
        print(f"   âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"   âœ… CUDA ë²„ì „: {torch.version.cuda}")
            print(f"   âœ… GPU ê°œìˆ˜: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"      - GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print(f"   â„¹ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ëŠë¦´ ìˆ˜ ìˆìŒ)")
    except Exception as e:
        print(f"   âŒ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ ì‹¤íŒ¨: {e}")
        return False
    
    # 2. ëª¨ë¸ ê²½ë¡œ í™•ì¸
    print(f"\n2. ëª¨ë¸ ê²½ë¡œ í™•ì¸...")
    model_path = Path(QWEN_MODEL_PATH)
    print(f"   ì„¤ì •ëœ ê²½ë¡œ: {QWEN_MODEL_PATH}")
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
    if not model_path.is_absolute():
        model_path = Path(__file__).parent / model_path
    
    if not model_path.exists():
        print(f"   âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        print(f"   ğŸ’¡ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
        print(f"   ğŸ’¡ Hugging Faceì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print(f"      - Qwen2.5-7B-Instruct: Qwen/Qwen2.5-7B-Instruct")
        print(f"      - Qwen2.5-3B-Instruct: Qwen/Qwen2.5-3B-Instruct")
        print(f"      - Qwen2-7B-Instruct: Qwen/Qwen2-7B-Instruct")
        print(f"\n   ğŸ’¡ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì˜ˆì‹œ:")
        print(f"      from transformers import AutoModelForCausalLM, AutoTokenizer")
        print(f"      model = AutoModelForCausalLM.from_pretrained('Qwen/Qwen2.5-7B-Instruct')")
        print(f"      tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2.5-7B-Instruct')")
        return False
    else:
        print(f"   âœ… ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ í™•ì¸: {model_path}")
    
    # 3. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    print(f"\n3. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸...")
    try:
        from models.qwen_local import QwenModel
        model = QwenModel(model_path=str(model_path))
        print(f"   ì‚¬ìš© ë””ë°”ì´ìŠ¤: {model.device}")
        print(f"   ëª¨ë¸ ë¡œë”© ì‹œì‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model.load_model()
        print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"\n   ìƒì„¸ ì˜¤ë¥˜:")
        traceback.print_exc()
        return False
    
    # 4. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
    print(f"\n4. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”!",
        "ë¹„íŠ¸ì½”ì¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "1+1ì€ ì–¼ë§ˆì¸ê°€ìš”?"
    ]
    
    for i, prompt in enumerate(test_prompts, 1):
        try:
            print(f"\n   í…ŒìŠ¤íŠ¸ {i}: {prompt}")
            print(f"   ìƒì„± ì¤‘...")
            
            # Qwen ëª¨ë¸ì˜ ê²½ìš° ì±„íŒ… í…œí”Œë¦¿ì„ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŒ
            response = model.generate(
                prompt=prompt,
                max_length=256,
                temperature=0.7
            )
            
            print(f"   âœ… ì‘ë‹µ:")
            print(f"   {response[:200]}{'...' if len(response) > 200 else ''}")
            
        except Exception as e:
            print(f"   âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    print(f"\n" + "=" * 60)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print("=" * 60)
    return True


if __name__ == "__main__":
    success = test_llm_connection()
    sys.exit(0 if success else 1)

