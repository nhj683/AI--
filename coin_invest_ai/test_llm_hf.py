"""
Hugging Faceì—ì„œ ì§ì ‘ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í…ŒìŠ¤íŠ¸
"""

import sys
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
try:
    import torch
    import transformers
    from transformers import AutoModelForCausalLM, AutoTokenizer
    PACKAGES_AVAILABLE = True
except ImportError as e:
    PACKAGES_AVAILABLE = False
    MISSING_PACKAGE = str(e)


def test_hf_model():
    """Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("Hugging Face ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    if not PACKAGES_AVAILABLE:
        print(f"\nâŒ í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print(f"   ì˜¤ë¥˜: {MISSING_PACKAGE}")
        return False
    
    # 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    print(f"\n1. ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸...")
    import torch
    import transformers
    print(f"   âœ… PyTorch ë²„ì „: {torch.__version__}")
    print(f"   âœ… Transformers ë²„ì „: {transformers.__version__}")
    print(f"   âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if not torch.cuda.is_available():
        print(f"   â„¹ï¸  CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤ (ëŠë¦´ ìˆ˜ ìˆìŒ)")
    
    # 2. ì‘ì€ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
    print(f"\n2. ì‘ì€ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)...")
    
    # Qwen2.5-3B-Instruct ë˜ëŠ” ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
    # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë” í° ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
    model_name = "Qwen/Qwen2.5-3B-Instruct"
    
    print(f"   ëª¨ë¸: {model_name}")
    print(f"   ğŸ’¡ ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print(f"   ğŸ’¡ ëª¨ë¸ í¬ê¸°: ì•½ 6GB (ë‹¤ìš´ë¡œë“œ í•„ìš”)")
    
    try:
        print(f"\n3. ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"   í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        print(f"   âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"   ëª¨ë¸ ë¡œë”© ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        if not torch.cuda.is_available():
            model = model.to("cpu")
        
        model.eval()
        print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # 4. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print(f"\n4. í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        test_prompts = [
            "ì•ˆë…•í•˜ì„¸ìš”!",
            "ë¹„íŠ¸ì½”ì¸ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "1+1ì€ ì–¼ë§ˆì¸ê°€ìš”?"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n   í…ŒìŠ¤íŠ¸ {i}: {prompt}")
            print(f"   ìƒì„± ì¤‘...")
            
            try:
                # Qwen2.5ëŠ” ì±„íŒ… í…œí”Œë¦¿ ì‚¬ìš©
                messages = [
                    {"role": "user", "content": prompt}
                ]
                
                # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
                text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # í† í¬ë‚˜ì´ì§•
                model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
                
                # ìƒì„±
                with torch.no_grad():
                    generated_ids = model.generate(
                        **model_inputs,
                        max_new_tokens=256,
                        temperature=0.7,
                        do_sample=True
                    )
                
                # ë””ì½”ë”©
                generated_ids = [
                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
                ]
                response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
                
                print(f"   âœ… ì‘ë‹µ:")
                print(f"   {response[:300]}{'...' if len(response) > 300 else ''}")
                
            except Exception as e:
                print(f"   âŒ í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                return False
        
        print(f"\n" + "=" * 60)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("=" * 60)
        return True
        
    except Exception as e:
        print(f"   âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        print(f"\nğŸ’¡ ëŒ€ì•ˆ:")
        print(f"   1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
        print(f"   2. ëª¨ë¸ ì´ë¦„ í™•ì¸: {model_name}")
        print(f"   3. ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (ì•½ 6GB í•„ìš”)")
        print(f"   4. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: Qwen/Qwen2.5-1.5B-Instruct)")
        return False


if __name__ == "__main__":
    success = test_hf_model()
    sys.exit(0 if success else 1)

